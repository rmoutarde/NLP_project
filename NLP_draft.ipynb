{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_draft.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rmoutarde/NLP_project/blob/main/NLP_draft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UL5Pm9RM8NdH"
      },
      "source": [
        "# NLP Project - Fake News Detection - Quentin MASCART & Raphaëlle VILLERS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXF1Wxwe8S2v"
      },
      "source": [
        "## Objective : \n",
        "\n",
        "## The data :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjvTJ4Es8mq9"
      },
      "source": [
        "## Project structure \n",
        "\n",
        "### 1. Preprocessing\n",
        "\n",
        "### 2. \n",
        "\n",
        "### 3. \n",
        "\n",
        "### ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0O4F4k-93BN"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtn5EFOC9L29"
      },
      "source": [
        "## Preparation\n",
        "\n",
        "Data can be manually downloaded here : https://www.kaggle.com/anmolkumar/fake-news-content-detection?fbclid=IwAR3a9gh9NHxedo3bNkTzErZpX12nk9lvjej1f608TzCcehfp2iIUmVH5pFc"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5374rq498gOn",
        "outputId": "d56d3c63-95f5-4d1a-fdae-832671ec3949"
      },
      "source": [
        "! wget https://raw.githubusercontent.com/rmoutarde/NLP_project/main/train.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-03-20 15:14:05--  https://raw.githubusercontent.com/rmoutarde/NLP_project/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1398534 (1.3M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.33M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2021-03-20 15:14:06 (14.3 MB/s) - ‘train.csv’ saved [1398534/1398534]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZqRoXkp1-Bbl",
        "outputId": "3ef416cd-bb86-49d9-e3f5-1e13c8671e0f"
      },
      "source": [
        "df_news = pd.read_csv(\"train.csv\")\n",
        "df_news.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Labels</th>\n",
              "      <th>Text</th>\n",
              "      <th>Text_Tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Says the Annies List political group supports ...</td>\n",
              "      <td>abortion</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>When did the decline of coal start? It started...</td>\n",
              "      <td>energy,history,job-accomplishments</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Hillary Clinton agrees with John McCain \"by vo...</td>\n",
              "      <td>foreign-policy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Health care reform legislation is likely to ma...</td>\n",
              "      <td>health-care</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>The economic turnaround started at the end of ...</td>\n",
              "      <td>economy,jobs</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Labels  ...                            Text_Tag\n",
              "0       1  ...                            abortion\n",
              "1       2  ...  energy,history,job-accomplishments\n",
              "2       3  ...                      foreign-policy\n",
              "3       1  ...                         health-care\n",
              "4       2  ...                        economy,jobs\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8UbvjPTDr4g",
        "outputId": "92dddb8c-b237-4d19-b406-0a962b2bf088"
      },
      "source": [
        "Fake=df_news[df_news.Labels==5].Text\n",
        "Fake"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5        The Chicago Bears have had more starting quart...\n",
              "16       McCain opposed a requirement that the governme...\n",
              "30                Says Paul Ryan is still endorsing Trump.\n",
              "35       We have a federal government that thinks they ...\n",
              "36       Austin is a city that has basically doubled in...\n",
              "                               ...                        \n",
              "10200    During the Bush administration, you actually h...\n",
              "10205    Nearly half of Hispanic voters in Arizona's la...\n",
              "10210    Since the Affordable Care Act passed, 90 perce...\n",
              "10211    Debt has almost doubled in Austin under Gov. P...\n",
              "10222    For the first time since the Korean War, total...\n",
              "Name: Text, Length: 1676, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BxKX3QG_SRW"
      },
      "source": [
        "Lables correspond to the following categorization :\n",
        "* Barely-True - 0\n",
        "* False – 1\n",
        "* Half-True – 2\n",
        "* Mostly-True - 3\n",
        "* Not-Known - 4\n",
        "* True - 5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryJr_2F8_DH5",
        "outputId": "cd9ed333-bbbf-4b89-951a-e4c0789662b0"
      },
      "source": [
        "df_news.Labels.value_counts()  "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2    2114\n",
              "1    1995\n",
              "3    1962\n",
              "5    1676\n",
              "0    1654\n",
              "4     839\n",
              "Name: Labels, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "eNtxVYDL_0qg",
        "outputId": "df2eab25-09f3-4a44-8a25-6f587aa93651"
      },
      "source": [
        "df_news.Labels.hist(bins = 11)\n",
        "plt.xlabel(\"Scope of truth\")\n",
        "plt.ylabel(\"Value counts\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Value counts')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvgM_XjjCozS"
      },
      "source": [
        "### Text_tag"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhXbDwWaAuFr",
        "outputId": "8399e9a4-bf42-4478-ddd9-753115d89118"
      },
      "source": [
        "# Processing the Text_tag column : split into list of words to categorize\n",
        "# We want to split according to the commas\n",
        "\n",
        "from functools import reduce\n",
        "from operator import add\n",
        "\n",
        "# List of words with separator = \",\"\n",
        "arr = df_news.Text_Tag.apply(lambda x: str(x).split(',')).array\n",
        "\n",
        "arr = reduce(add, arr)\n",
        "print(\"number of words =\", len(arr))\n",
        "print(\"nb of different words =\", len(set(arr)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of words = 22205\n",
            "nb of different words = 143\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArwnrgkpDFSS"
      },
      "source": [
        "### Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zhVfTd_XEeft"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import ToktokTokenizer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cksgr2XWENh5",
        "outputId": "7902e925-182f-445e-d6d1-eb6d6527114f"
      },
      "source": [
        "# Intuition : maybe Fake news use less vocabulary than True news\n",
        "\n",
        "tknr = TweetTokenizer()\n",
        "for label in range(6):\n",
        "  arr = df_news[df_news.Labels == label].Text.apply(lambda x: tknr.tokenize(x)).array\n",
        "  arr = reduce(add, arr)\n",
        "  # the set object transforms a list to the set of unique elements in the list :\n",
        "  print(\"For label {0}, set of different words with tokenizer {1} is {2} :\".format(label, tknr, len(set(arr))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 0, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 5796 :\n",
            "For label 1, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 6239 :\n",
            "For label 2, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 6454 :\n",
            "For label 3, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 6116 :\n",
            "For label 4, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 3971 :\n",
            "For label 5, set of different words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c197a310> is 5596 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbORy4zKGUbc"
      },
      "source": [
        "Remember 0 is Barely True and 5 is True.\n",
        "We normalize by total amount of words, or by text count in each category to get a clearer idea."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdImwxuKGf21",
        "outputId": "c4d99907-587a-4e15-bdef-c7d3e659ad0d"
      },
      "source": [
        "# Normalizing by number of words in each label category\n",
        "\n",
        "tknr = TweetTokenizer()\n",
        "for label in range(6):\n",
        "  arr = df_news[df_news.Labels == label].Text.apply(lambda x: tknr.tokenize(x)).array\n",
        "  arr = reduce(add, arr)\n",
        "  # the set object transforms a list to the set of unique elements in the list :\n",
        "  print(\"For label {0}, set of different words over number of words with tokenizer {1} is {2} :\".\n",
        "        format(label, tknr, len(set(arr))/len(arr)))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 0, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.17048061650685334 :\n",
            "For label 1, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.16274944567627495 :\n",
            "For label 2, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.14390831252229755 :\n",
            "For label 3, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.1493820526598603 :\n",
            "For label 4, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.24314229733039433 :\n",
            "For label 5, set of different words over number of words with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c0c07090> is 0.16401899290696992 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNbh-evHHKLq",
        "outputId": "49a2d839-a153-4525-ef23-3db0faeb5c78"
      },
      "source": [
        "df_news[df_news.Labels == 1].Text.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1995"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "halChUKfG-NB",
        "outputId": "d3bfd2db-7089-4490-97eb-d40c92ac4d68"
      },
      "source": [
        "# Normalizing by text count in each label category\n",
        "\n",
        "tknr = TweetTokenizer()\n",
        "for label in range(6):\n",
        "  arr = df_news[df_news.Labels == label].Text.apply(lambda x: tknr.tokenize(x)).array\n",
        "  arr = reduce(add, arr)\n",
        "  text_count = df_news[df_news.Labels == label].Text.count()\n",
        "  # the set object transforms a list to the set of unique elements in the list :\n",
        "  print(\"For label {0}, set of different words over text count with tokenizer {1} is {2} :\".\n",
        "        format(label, tknr, len(set(arr))/text_count))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 0, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 3.5042321644498187 :\n",
            "For label 1, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 3.1273182957393484 :\n",
            "For label 2, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 3.052980132450331 :\n",
            "For label 3, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 3.1172273190621813 :\n",
            "For label 4, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 4.733015494636472 :\n",
            "For label 5, set of different words over text count with tokenizer <nltk.tokenize.casual.TweetTokenizer object at 0x7f61c8bf8350> is 3.3389021479713605 :\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caTzlizpHogL"
      },
      "source": [
        "I don't think we can conclude... \n",
        "\n",
        "Another assumption : maybe there are more numbers in True news ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toTRQ9oiKkj1"
      },
      "source": [
        "import re\n",
        "# Regular expression that matches any sequence of numbers:\n",
        "nb =  '[0-9]+' # retrieve all numbers : ([0-9] == from 0 to 9, and + allows multiple matches)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RX6nJuNMK0kX"
      },
      "source": [
        "# Regular expression that matches any sequence of numbers:\n",
        "#ints, floats, percents, years\n",
        "\n",
        "# compile : Compile a regular expression pattern into a regular expression object,\n",
        "# which can be used for matching using its match(), search() and other methods\n",
        "ints_prog = re.compile('[0-9]+')\n",
        "floats_prog = re.compile('[0-9]+\\.?[0-9]*') # \\. pour désigner le point, * pour répétitions zero ou plusieurs fois\n",
        "percents_prog = re.compile('[0-9]+\\.?[0-9]*%')\n",
        "\n",
        "years_prog = re.compile('[12][089][0-9][0-9]') # a year between 1800 and 2099\n",
        "\n",
        "# Compute number of matches for every abstract\n",
        "progs = [ints_prog, floats_prog, percents_prog, years_prog]\n",
        "nb_types = [\"ints\", \"floats\", \"percents\", \"years\"]\n",
        "\n",
        "df_nb = df_news  # Don't forget to compute over unique abstracts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "TuCZhAIqLG_M",
        "outputId": "a47bb26a-e482-43d3-9ca4-57410fceae4f"
      },
      "source": [
        "for prog, nb_type in zip(progs, nb_types) :\n",
        "  #Create 1 column per type\n",
        "  #Split string by the occurrences of pattern => splits - 1 gives number of occurences\n",
        "  df_nb[nb_type] = pd.DataFrame(df_nb['Text'].apply(lambda x: len(prog.split(x))-1).array)\n",
        "df_nb[nb_types].head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ints</th>\n",
              "      <th>floats</th>\n",
              "      <th>percents</th>\n",
              "      <th>years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   ints  floats  percents  years\n",
              "0     0       0         0      0\n",
              "1     0       0         0      0\n",
              "2     0       0         0      0\n",
              "3     0       0         0      0\n",
              "4     0       0         0      0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfKH6E5rLdhK",
        "outputId": "f7155914-fd47-45f1-b2a6-55b09fc9d53d"
      },
      "source": [
        "print(df_nb.ints.sum())\n",
        "print(df_nb.floats.sum())\n",
        "print(df_nb.percents.sum())\n",
        "print(df_nb.years.sum())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7317\n",
            "6951\n",
            "110\n",
            "915\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1mbaUTPTLtMT"
      },
      "source": [
        "df_nb['numbers'] = df_nb.ints +df_nb.floats + df_nb.percents + df_nb.years \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yruIT2Y1MwMJ",
        "outputId": "7a6e05f5-188e-4d13-8ca4-429a6c704741"
      },
      "source": [
        "for label in range(6):\n",
        "  nb_count = df_nb[df_nb.Labels == label].numbers.sum()\n",
        "  text_count = df_news[df_news.Labels == label].Text.count()\n",
        "  # the set object transforms a list to the set of unique elements in the list :\n",
        "  print(\"For label {0}, there are {1} numbers over {2} texts, ratio is {3}\".\n",
        "        format(label, nb_count, text_count, nb_count/text_count))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "For label 0, there are 2250 numbers over 1654 texts, ratio is 1.3603385731559854\n",
            "For label 1, there are 2410 numbers over 1995 texts, ratio is 1.2080200501253133\n",
            "For label 2, there are 3544 numbers over 2114 texts, ratio is 1.6764427625354779\n",
            "For label 3, there are 3600 numbers over 1962 texts, ratio is 1.834862385321101\n",
            "For label 4, there are 814 numbers over 839 texts, ratio is 0.9702026221692491\n",
            "For label 5, there are 2675 numbers over 1676 texts, ratio is 1.5960620525059666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDM4TyWLNs7W"
      },
      "source": [
        "To be explored further to check whether it is significant but there might be something there !\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfnK4iq-S7BG"
      },
      "source": [
        "#### Classic NLP Pipeline[texte du lien](https://)\n",
        "\n",
        "##### Tokenization using spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_UHNFtfS9sx",
        "outputId": "6e34cb1c-3b64-46e8-ada6-1b2f40aec27b"
      },
      "source": [
        "#---------- Tokenization using spacy -----------#\n",
        "\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Instanciating the tokenizer\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "\n",
        "sentence = nlp(df_news.Text[29]) \n",
        "for word in sentence:\n",
        "    print(word.text)\n",
        "print(len(sentence))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Youth\n",
            "unemployment\n",
            "in\n",
            "minority\n",
            "communities\n",
            "is\n",
            "about\n",
            "40\n",
            "to\n",
            "45\n",
            "percent\n",
            ".\n",
            "12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0UDxP3lTUgn",
        "outputId": "ef374bab-2ff2-4899-c5c0-1530f6eacc00"
      },
      "source": [
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "arr_spacy = df_news.Text.apply(lambda x : [w.text for w in tokenizer(x)]).array\n",
        "arr_spacy = reduce(add, arr_spacy)\n",
        "\n",
        "print(\"Total nb of words with Spacy {0} and Vocabulary size {1}\".format(len(arr_spacy), len(set(arr_spacy))))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total nb of words with Spacy 184842 and Vocabulary size 21682\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p014SCwrTbfm",
        "outputId": "720b37f4-c388-4121-f425-5b272d69c29e"
      },
      "source": [
        "# Inspecting a tokenization sample\n",
        "tokenization_sample = df_news.Text.head().apply(lambda x: \n",
        "                                              [w.text for w in tokenizer(x)])\n",
        "print(\"Tokenization sample:\\n {}\".format(tokenization_sample))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenization sample:\n",
            " 0    [Says, the, Annies, List, political, group, su...\n",
            "1    [When, did, the, decline, of, coal, start?, It...\n",
            "2    [Hillary, Clinton, agrees, with, John, McCain,...\n",
            "3    [Health, care, reform, legislation, is, likely...\n",
            "4    [The, economic, turnaround, started, at, the, ...\n",
            "Name: Text, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTVeDgYRThfG"
      },
      "source": [
        "##### Lemmatize using Spacy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1J6tqh49TgOg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "246a65fe-a476-4958-ecb4-c59ad8253986"
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "from spacy.lemmatizer import Lemmatizer\n",
        "from spacy.lookups import Lookups\n",
        "\n",
        "#from spacy.lemmatizer import Lemmatize\n",
        "from spacy.lang.en import English\n",
        "lemmatizer = English.Defaults.create_lemmatizer()\n",
        "#from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
        "\n",
        "# Instanciating lemmatizer\n",
        "#lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
        "lemmatizer = nlp.Defaults.create_lemmatizer()\n",
        "print('loaded')\n",
        "# Usage: lemmatizer(token, POS(token))\n",
        "# But we will rather use spacy's nlp engine\n",
        "\n",
        "# Counting lemmas vocabulary count\n",
        "\n",
        "# Usage: lemmatizer(token, POS(token))\n",
        "# But we will rather use spacy's nlp engine\n",
        "\n",
        "pipeline = [Lemmatizer]\n",
        "\n",
        "tokenizer = Tokenizer(nlp.vocab)\n",
        "arr_lemmatizer = df_news.Text.apply(lambda x : [w.lemma_ for w in nlp(x)]).array\n",
        "arr_lemmatizer = reduce(add, arr_lemmatizer)\n",
        "\n",
        "print(\"Total nb of words with Lemmatizer {0} and Vocabulary size {1}\".format(len(arr_lemmatizer), len(set(arr_lemmatizer))))\n",
        "\n",
        "\n",
        "# Inspecting a tokenization sample\n",
        "lemmatizer_sample = df_news.Text.head().apply(lambda x: \n",
        "                                              [w.lemma_ for w in nlp(x)])\n",
        "print(\"Tokenization sample:\\n {}\".format(lemmatizer_sample))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc0VPemRDKIk"
      },
      "source": [
        "### Is there any relationship between the categories in Text_tag and the length of text ? The truth of text ? The proportion of numbers quoted in text ? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xnaabe86DIF7"
      },
      "source": [
        "# We might want to detect quotes and numbers (percentage, years...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-tEk1K9DqON"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkmaY0cXAuJL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}